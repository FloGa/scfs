//! # SCFS – SplitCatFS
//!
//! A convenient splitting and concatenating filesystem.
//!
//! ## Motivation
//!
//! ### History
//!
//! While setting up a cloud based backup and archive solution, I encountered the
//! following phenomenon: Many small files would get uploaded quite fast and –
//! depending on the actual cloud storage provider – highly concurrently, while
//! big files tend to slow down the whole process. The explanation is simple, most
//! cloud storage providers do not support the upload of a single file, sometimes
//! they would not even support resuming a partial upload. You would need to
//! upload it in one go, sequentially byte for byte, it's all or nothing.
//!
//! Now consider a scenario, where you upload a really big file, like a mirror of
//! your Raspberry Pi's SD card with the system and configuration on it. I have
//! such a file, it is about 4 GB big. Now, while backing up my system, this was
//! the last file to be uploaded. According to ETA calculations, it would have
//! taken several hours, so I let it run overnight. The next morning I found out
//! that after around 95% of upload process, my internet connection vanished for
//! just a few seconds, but long enough that the transfer tool aborted the upload.
//! The temporary file got deleted from the cloud storage, so I had to start from
//! zero again. Several hours of uploading wasted.
//!
//! I thought of a way to split big files, so that I can upload it more
//! efficiently, but I came to the conclusion, that manually splitting files,
//! uploading them, and deleting them afterwards locally, is not a very scalable
//! solution.
//!
//! So I came up with the idea of a special filesystem. A filesystem that would
//! present big files as if they were many small chunks in separate files. In
//! reality, the chunks would all point to the same physical file, only with
//! different offsets. This way I could upload chunked files in parallel without
//! losing too much progress, even if the upload gets aborted midway.
//!
//! *SplitFS* was born.
//!
//! If I download such chunked file parts, I would need to call `cat * >file`
//! afterwards to re-create the actual file. This seems like a similar hassle like
//! manually splitting files. That's why I had also *CatFS* in mind, when
//! developing SCFS. CatFS will concatenate chunked files transparently and
//! present them as a complete files.
//!
//! CatFS is included in SCFS since version 0.4.0.
//!
//!
//! ### Why Rust?
//!
//! I am relatively new to Rust and I thought, the best way to deepen my
//! understanding with Rust is to take on a project that would require dedication
//! and a certain knowledge of the language.
//!
//! ## Installation
//!
//! SCFS can be installed easily through Cargo via `crates.io`:
//!
//!     cargo install scfs
//!
//! ## Usage
//!
//! ### SplitFS
//!
//! To mount a directory with SplitFS, use the following form:
//!
//!     scfs --mode=split <base directory> <mount point>
//!
//! The directory specified as `mount point` will now reflect the content of `base
//! directory`, replacing each regular file with a directory that contains
//! enumerated chunks of that file as separate files.
//!
//! ### CatFS
//!
//! To mount a directory with CatFS, use the following form:
//!
//!     scfs --mode=cat <base directory> <mount point>
//!
//! Please note that `base directory` needs to be a directory structure that has
//! been generated by SplitFS.
//!
//! The directory specified as `mount point` will now reflect the content of `base
//! directory`, replacing each directory with chunked files in it as single files.
//!
//! ## Limitations
//!
//! Please be aware that this project is merely a raw prototype for now!
//! Specifically:
//!
//! -   It only works on Linux for now, maybe even on UNIX. But definitely not on
//!     Windows or MacOS.
//!
//! -   It can only work with directories and regular files. Every other file type
//!     will be ignored or may end end up in a `panic!`.
//!
//! -   The base directory will be mounted read-only in the new mount point and it
//!     is expected that it will not be altered while mounted.
//!
//! -   *CatFS* will not check if `base directory` is a valid SplitFS structure.
//!     This might change in the future. For now, please use CatFS *only* on
//!     directory structures that have been generated by SplitFS.

use std::collections::HashMap;
use std::ffi::{OsStr, OsString};
use std::fs;
use std::fs::{File, Metadata};
use std::io::{BufReader, Read, Seek, SeekFrom};
use std::os::linux::fs::MetadataExt;
use std::path::Path;

use fuse::{
    FileAttr, FileType, Filesystem, ReplyAttr, ReplyData, ReplyDirectory, ReplyEmpty, ReplyEntry,
    ReplyOpen, Request,
};
use libc::ENOENT;
use rusqlite::{params, Connection, Error, Row, NO_PARAMS};
use threadpool::ThreadPool;
use time::Timespec;

const TTL: Timespec = Timespec {
    sec: 60 * 60 * 24,
    nsec: 0,
};

const STMT_CREATE: &str = "CREATE TABLE Files (
    ino INTEGER PRIMARY KEY,
    parent_ino INTEGER,
    path TEXT UNIQUE,
    part INTEGER,
    vdir INTEGER
    )";
const STMT_INSERT: &str =
    "INSERT INTO Files (ino, parent_ino, path, part, vdir) VALUES (?, ?, ?, ?, ?)";
const STMT_QUERY_BY_INO: &str = "SELECT * FROM Files WHERE ino = ?";
const STMT_QUERY_BY_PARENT_INO: &str = "SELECT * FROM Files WHERE parent_ino = ? LIMIT -1 OFFSET ?";
const STMT_QUERY_LAST_INO: &str = "SELECT * FROM Files ORDER BY _rowid_ DESC LIMIT 1";

const BLOCK_SIZE: u64 = 2 * 1024 * 1024;

fn convert_filetype(ft: fs::FileType) -> FileType {
    if ft.is_dir() {
        FileType::Directory
    } else if ft.is_file() {
        FileType::RegularFile
    } else if ft.is_symlink() {
        FileType::Symlink
    } else {
        panic!("Not supported")
    }
}

fn convert_metadata_to_attr(meta: Metadata, ino: u64) -> FileAttr {
    FileAttr {
        ino: if ino != 0 { ino } else { meta.st_ino() },
        size: meta.st_size(),
        blocks: meta.st_blocks(),
        atime: Timespec::new(meta.st_atime(), meta.st_atime_nsec() as i32),
        mtime: Timespec::new(meta.st_mtime(), meta.st_mtime_nsec() as i32),
        ctime: Timespec::new(meta.st_ctime(), meta.st_ctime_nsec() as i32),
        crtime: Timespec::new(0, 0),
        kind: convert_filetype(meta.file_type()),
        perm: meta.st_mode() as u16,
        nlink: meta.st_nlink() as u32,
        uid: meta.st_uid(),
        gid: meta.st_gid(),
        rdev: meta.st_rdev() as u32,
        flags: 0,
    }
}

pub struct SplitFS {
    file_db: Connection,
    file_handles: HashMap<u64, FileHandle>,
    threadpool: ThreadPool,
}

pub struct CatFS {
    file_db: Connection,
    file_handles: HashMap<u64, Vec<FileHandle>>,
    threadpool: ThreadPool,
}

struct FileHandle {
    file: OsString,
    start: u64,
    end: u64,
}

#[derive(Debug)]
struct FileInfo {
    ino: u64,
    parent_ino: u64,
    path: OsString,
    part: u64,
    vdir: bool,
}

impl FileInfo {
    fn with_ino(ino: u64) -> Self {
        FileInfo {
            ino,
            parent_ino: Default::default(),
            path: Default::default(),
            part: 0,
            vdir: false,
        }
    }

    fn with_parent_ino(parent_ino: u64) -> Self {
        FileInfo {
            ino: Default::default(),
            parent_ino,
            path: Default::default(),
            part: 0,
            vdir: false,
        }
    }
}

impl From<&Row<'_>> for FileInfo {
    fn from(row: &Row) -> Self {
        FileInfoRow::from(row).into()
    }
}

#[derive(Debug)]
struct FileInfoRow {
    ino: i64,
    parent_ino: i64,
    path: String,
    part: i64,
    vdir: bool,
}

impl From<&Row<'_>> for FileInfoRow {
    fn from(row: &Row) -> Self {
        FileInfoRow {
            ino: row.get(0).unwrap(),
            parent_ino: row.get(1).unwrap(),
            path: row.get(2).unwrap(),
            part: row.get(3).unwrap(),
            vdir: row.get(4).unwrap(),
        }
    }
}

impl From<FileInfoRow> for FileInfo {
    fn from(f: FileInfoRow) -> Self {
        FileInfo {
            ino: f.ino as u64,
            parent_ino: f.parent_ino as u64,
            path: OsString::from(f.path),
            part: f.part as u64,
            vdir: f.vdir,
        }
    }
}

impl From<FileInfo> for FileInfoRow {
    fn from(f: FileInfo) -> Self {
        FileInfoRow {
            ino: f.ino as i64,
            parent_ino: f.parent_ino as i64,
            path: f.path.into_string().unwrap(),
            part: f.part as i64,
            vdir: f.vdir,
        }
    }
}

impl SplitFS {
    pub fn new(mirror: OsString) -> Self {
        let file_db = Connection::open_in_memory().unwrap();

        file_db.execute(STMT_CREATE, NO_PARAMS).unwrap();

        SplitFS::populate(&file_db, &mirror, 0);

        let file_handles = Default::default();

        let threadpool = Default::default();

        SplitFS {
            file_db,
            file_handles,
            threadpool,
        }
    }

    fn get_file_info_from_ino(&self, ino: u64) -> Result<FileInfo, Error> {
        let ino = FileInfoRow::from(FileInfo::with_ino(ino)).ino;

        let mut stmt = self.file_db.prepare_cached(STMT_QUERY_BY_INO).unwrap();

        let file_info = stmt
            .query_map(params![ino], |row| Ok(FileInfo::from(row)))?
            .next()
            .unwrap();
        file_info
    }

    fn get_file_info_from_parent_ino_and_file_name(
        &self,
        parent_ino: u64,
        file_name: OsString,
    ) -> Result<FileInfo, Error> {
        let parent_ino = FileInfoRow::from(FileInfo::with_parent_ino(parent_ino)).parent_ino;

        let mut stmt = self
            .file_db
            .prepare_cached(STMT_QUERY_BY_PARENT_INO)
            .unwrap();

        let inos = stmt
            .query_map(params![parent_ino, 0], |row| Ok(FileInfo::from(row).ino))
            .unwrap();

        let file_info = inos
            .map(|ino| {
                let ino = ino.unwrap();
                self.get_file_info_from_ino(ino).unwrap()
            })
            .skip_while(|file_info| {
                let name_from_db = Path::new(&file_info.path).file_name().unwrap();
                let name_to_look_for = Path::new(&file_name).file_name().unwrap();
                name_from_db != name_to_look_for
            })
            .next();

        match file_info {
            Some(f) => Ok(f),
            None => Err(Error::QueryReturnedNoRows),
        }
    }

    fn get_attr_from_file_info(&self, file_info: &FileInfo) -> FileAttr {
        if file_info.part == 0 {
            let mut attr =
                convert_metadata_to_attr(fs::metadata(&file_info.path).unwrap(), file_info.ino);
            attr.kind = FileType::Directory;
            attr.blocks = 0;
            attr.perm = 0o755;
            attr
        } else {
            let mut attr = convert_metadata_to_attr(
                fs::metadata(
                    self.get_file_info_from_ino(file_info.parent_ino)
                        .unwrap()
                        .path,
                )
                .unwrap(),
                file_info.ino,
            );
            attr.size = u64::min(BLOCK_SIZE, attr.size - (file_info.part - 1) * BLOCK_SIZE);
            attr
        }
    }

    fn populate<P: AsRef<Path>>(file_db: &Connection, path: P, parent_ino: u64) {
        let path = path.as_ref();

        let mut attr = convert_metadata_to_attr(path.metadata().unwrap(), 0);

        attr.ino = if parent_ino == 0 {
            1
        } else {
            file_db
                .prepare_cached(STMT_QUERY_LAST_INO)
                .unwrap()
                .query_map(NO_PARAMS, |row| Ok(FileInfo::from(row).ino))
                .unwrap()
                .next()
                .unwrap()
                .unwrap()
                + 1
        };

        let file_info = FileInfoRow::from(FileInfo {
            ino: attr.ino,
            parent_ino,
            path: OsString::from(path),
            part: 0,
            vdir: attr.kind == FileType::RegularFile,
        });

        file_db
            .prepare_cached(STMT_INSERT)
            .unwrap()
            .execute(params![
                file_info.ino,
                file_info.parent_ino,
                file_info.path,
                file_info.part,
                file_info.vdir
            ])
            .unwrap();

        if let FileType::RegularFile = attr.kind {
            let blocks = f64::ceil(attr.size as f64 / BLOCK_SIZE as f64) as u64;
            for i in 0..blocks {
                let file_info = FileInfoRow::from(FileInfo {
                    ino: attr.ino + i + 1,
                    parent_ino: attr.ino,
                    path: OsString::from(path.join(format!("scfs.{:010}", i))),
                    part: i + 1,
                    vdir: false,
                });

                file_db
                    .prepare_cached(STMT_INSERT)
                    .unwrap()
                    .execute(params![
                        file_info.ino,
                        file_info.parent_ino,
                        file_info.path,
                        file_info.part,
                        file_info.vdir
                    ])
                    .unwrap();
            }
        }

        if path.is_dir() {
            for entry in fs::read_dir(path).unwrap() {
                let entry = entry.unwrap();
                SplitFS::populate(&file_db, entry.path(), attr.ino);
            }
        }
    }
}

impl Filesystem for SplitFS {
    fn lookup(&mut self, _req: &Request, parent: u64, name: &OsStr, reply: ReplyEntry) {
        let file_info =
            self.get_file_info_from_parent_ino_and_file_name(parent, OsString::from(name));
        if let Ok(file_info) = file_info {
            let attr = self.get_attr_from_file_info(&file_info);
            reply.entry(&TTL, &attr, 0);
        } else {
            reply.error(ENOENT);
        }
    }

    fn getattr(&mut self, _req: &Request, ino: u64, reply: ReplyAttr) {
        let file_info = self.get_file_info_from_ino(ino);
        if let Ok(file_info) = file_info {
            let attr = self.get_attr_from_file_info(&file_info);
            reply.attr(&TTL, &attr)
        } else {
            reply.error(ENOENT)
        }
    }

    fn open(&mut self, _req: &Request, ino: u64, _flags: u32, reply: ReplyOpen) {
        let file_info = self.get_file_info_from_ino(ino);
        if let Ok(file_info) = file_info {
            let file = self
                .get_file_info_from_ino(file_info.parent_ino)
                .unwrap()
                .path;

            let start = (file_info.part - 1) * BLOCK_SIZE;
            let end = start + BLOCK_SIZE;
            let fh = time::precise_time_ns();

            self.file_handles
                .insert(fh, FileHandle { file, start, end });

            reply.opened(fh, 0);
        } else {
            reply.error(ENOENT)
        }
    }

    fn read(
        &mut self,
        _req: &Request,
        _ino: u64,
        fh: u64,
        offset: i64,
        size: u32,
        reply: ReplyData,
    ) {
        let offset = offset as u64;
        let size = size as u64;

        let handle = self.file_handles.get(&fh).unwrap();
        let file = handle.file.clone();

        let offset = offset.min(handle.end - handle.start);
        let size = size.min(handle.end - handle.start - offset);
        let start = handle.start;

        self.threadpool.execute(move || {
            let mut file = BufReader::new(File::open(file).unwrap());

            file.seek(SeekFrom::Start(start + offset)).unwrap();

            let bytes = file
                .take(size)
                .bytes()
                .map(|b| b.unwrap())
                .collect::<Vec<_>>();

            reply.data(&bytes);
        });
    }

    fn release(
        &mut self,
        _req: &Request,
        _ino: u64,
        fh: u64,
        _flags: u32,
        _lock_owner: u64,
        _flush: bool,
        reply: ReplyEmpty,
    ) {
        self.file_handles.remove(&fh);
        reply.ok();
    }

    fn readdir(
        &mut self,
        _req: &Request,
        ino: u64,
        _fh: u64,
        offset: i64,
        mut reply: ReplyDirectory,
    ) {
        let file_info = self.get_file_info_from_ino(ino);

        if let Ok(file_info) = file_info {
            if offset < 2 {
                if offset == 0 {
                    reply.add(file_info.ino, 1, FileType::Directory, ".");
                }
                reply.add(
                    if file_info.parent_ino == 0 {
                        file_info.ino
                    } else {
                        file_info.parent_ino
                    },
                    2,
                    FileType::Directory,
                    "..",
                );
            }

            let mut stmt = self
                .file_db
                .prepare_cached(STMT_QUERY_BY_PARENT_INO)
                .unwrap();
            let items = stmt
                .query_map(
                    params![
                        FileInfoRow::from(FileInfo::with_parent_ino(file_info.ino)).parent_ino,
                        // The offset includes . and .., both which are not included in the
                        // database, so the SELECT offset must be adjusted. Since the offset could
                        // be negative, set it to 0 in that case.
                        0.max(offset - 2)
                    ],
                    |row| Ok(FileInfo::from(row)),
                )
                .unwrap();
            for (off, item) in items.enumerate() {
                let item = item.unwrap();

                // Here the item is added to the directory listing. It is important to note that
                // the offset parameter is quite crucial for correct function. The offset parameter
                // is used for succeeding calls to start with the next item after the last item
                // from the previous call. So, the offset parameter has to be one more than the
                // index of the current item. Furthermore, since "." and ".." have been added
                // manually as to not pollute the database with them, they also have to be handled
                // properly. They always get inserted in positions 0 and 1 respectively. If the
                // call starts at offset 0, then both of the directory hardlinks are included and
                // the offset must be increased by 2. If the starting offset is 1, then only "."
                // has been already added. For the additional "..", the offset has to be increased
                // by 1. If the offset is greater than 1, then the hardlinks have been taken care
                // of and the offset is already correct.
                reply.add(
                    item.ino,
                    offset
                        + if offset == 0 {
                            2
                        } else if offset == 1 {
                            1
                        } else {
                            0
                        }
                        + off as i64
                        + 1,
                    if item.part > 0 {
                        FileType::RegularFile
                    } else {
                        FileType::Directory
                    },
                    Path::new(&item.path).file_name().unwrap(),
                );
            }
            reply.ok();
        } else {
            reply.error(ENOENT);
        }
    }
}

impl CatFS {
    pub fn new(mirror: OsString) -> Self {
        let file_db = Connection::open_in_memory().unwrap();

        file_db.execute(STMT_CREATE, NO_PARAMS).unwrap();

        CatFS::populate(&file_db, &mirror, 0);

        {
            let query = "UPDATE Files SET vdir = 1
                 WHERE ino IN (
                    SELECT DISTINCT parent_ino FROM Files WHERE part != 0
                )";
            let mut stmt = file_db.prepare(query).unwrap();
            stmt.execute(NO_PARAMS).unwrap();
        }

        let file_handles = Default::default();

        let threadpool = Default::default();

        CatFS {
            file_db,
            file_handles,
            threadpool,
        }
    }

    fn get_file_info_from_ino(&self, ino: u64) -> Result<FileInfo, Error> {
        let ino = FileInfoRow::from(FileInfo::with_ino(ino)).ino;

        let mut stmt = self.file_db.prepare_cached(STMT_QUERY_BY_INO).unwrap();

        let file_info = stmt
            .query_map(params![ino], |row| Ok(FileInfo::from(row)))?
            .next()
            .unwrap();
        file_info
    }

    fn get_files_info_from_parent_ino(&self, parent_ino: u64) -> Vec<FileInfo> {
        let parent_ino = FileInfoRow::from(FileInfo::with_parent_ino(parent_ino)).parent_ino;

        let mut stmt = self
            .file_db
            .prepare_cached(STMT_QUERY_BY_PARENT_INO)
            .unwrap();

        stmt.query_map(params![parent_ino, 0], |row| Ok(FileInfo::from(row)))
            .unwrap()
            .map(|res| res.unwrap())
            .collect()
    }

    fn get_file_info_from_parent_ino_and_file_name(
        &self,
        parent_ino: u64,
        file_name: OsString,
    ) -> Result<FileInfo, Error> {
        let parent_ino = FileInfoRow::from(FileInfo::with_parent_ino(parent_ino)).parent_ino;

        let mut stmt = self
            .file_db
            .prepare_cached(STMT_QUERY_BY_PARENT_INO)
            .unwrap();

        let inos = stmt
            .query_map(params![parent_ino, 0], |row| Ok(FileInfo::from(row).ino))
            .unwrap();

        let file_info = inos
            .map(|ino| {
                let ino = ino.unwrap();
                self.get_file_info_from_ino(ino).unwrap()
            })
            .skip_while(|file_info| {
                let name_from_db = Path::new(&file_info.path).file_name().unwrap();
                let name_to_look_for = Path::new(&file_name).file_name().unwrap();
                name_from_db != name_to_look_for
            })
            .next();

        match file_info {
            Some(f) => Ok(f),
            None => Err(Error::QueryReturnedNoRows),
        }
    }

    fn get_attr_from_file_info(&self, file_info: &FileInfo) -> FileAttr {
        if file_info.vdir {
            let parts = self.get_files_info_from_parent_ino(file_info.ino);
            let attrs = parts
                .iter()
                .map(|info| convert_metadata_to_attr(fs::metadata(&info.path).unwrap(), info.ino))
                .collect::<Vec<_>>();
            let mut attr = attrs.get(0).unwrap().clone();
            attr.ino = file_info.ino;
            attr.blocks = attrs.iter().map(|attr| attr.blocks).sum();
            attr.size = attrs.iter().map(|attr| attr.size).sum();
            attr
        } else {
            let attr =
                convert_metadata_to_attr(fs::metadata(&file_info.path).unwrap(), file_info.ino);
            attr
        }
    }

    fn populate<P: AsRef<Path>>(file_db: &Connection, path: P, parent_ino: u64) {
        let path = path.as_ref();

        let ino = if parent_ino == 0 {
            1
        } else {
            file_db
                .prepare_cached(STMT_QUERY_LAST_INO)
                .unwrap()
                .query_map(NO_PARAMS, |row| Ok(FileInfo::from(row).ino))
                .unwrap()
                .next()
                .unwrap()
                .unwrap()
                + 1
        };

        let file_info = FileInfoRow::from(FileInfo {
            ino,
            parent_ino,
            path: OsString::from(path),
            part: if path.is_file() {
                path.file_name().unwrap().to_str().unwrap()[5..]
                    .parse::<u64>()
                    .unwrap()
                    + 1
            } else {
                0
            },
            vdir: false,
        });

        file_db
            .prepare_cached(STMT_INSERT)
            .unwrap()
            .execute(params![
                file_info.ino,
                file_info.parent_ino,
                file_info.path,
                file_info.part,
                file_info.vdir
            ])
            .unwrap();

        if path.is_dir() {
            for entry in fs::read_dir(path).unwrap() {
                let entry = entry.unwrap();
                CatFS::populate(&file_db, entry.path(), ino);
            }
        }
    }
}

impl Filesystem for CatFS {
    fn lookup(&mut self, _req: &Request, parent: u64, name: &OsStr, reply: ReplyEntry) {
        let file_info =
            self.get_file_info_from_parent_ino_and_file_name(parent, OsString::from(name));
        if let Ok(file_info) = file_info {
            let attr = self.get_attr_from_file_info(&file_info);
            reply.entry(&TTL, &attr, 0);
        } else {
            reply.error(ENOENT);
        }
    }

    fn getattr(&mut self, _req: &Request, ino: u64, reply: ReplyAttr) {
        let file_info = self.get_file_info_from_ino(ino);
        if let Ok(file_info) = file_info {
            let attr = self.get_attr_from_file_info(&file_info);
            reply.attr(&TTL, &attr)
        } else {
            reply.error(ENOENT)
        }
    }

    fn open(&mut self, _req: &Request, ino: u64, _flags: u32, reply: ReplyOpen) {
        let files = self.get_files_info_from_parent_ino(ino);

        let fhs = files
            .iter()
            .map(|file| FileHandle {
                file: file.path.clone(),
                start: 0,
                end: 0,
            })
            .collect();

        let fh = time::precise_time_ns();
        self.file_handles.insert(fh, fhs);
        reply.opened(fh, 0);
    }

    fn read(
        &mut self,
        _req: &Request,
        ino: u64,
        fh: u64,
        offset: i64,
        size: u32,
        reply: ReplyData,
    ) {
        let offset = offset as usize;
        let size = size as usize;

        let file_size = self
            .get_attr_from_file_info(&self.get_file_info_from_ino(ino).unwrap())
            .size as usize;

        let offset = offset.min(file_size);
        let size = size.min(file_size - offset);

        let part_start = offset / BLOCK_SIZE as usize;
        let part_end = (offset + size) / BLOCK_SIZE as usize;

        let files = (part_start..=part_end)
            .map(|part| {
                self.file_handles
                    .get(&fh)
                    .unwrap()
                    .get(part)
                    .unwrap()
                    .file
                    .clone()
            })
            .collect::<Vec<_>>();

        self.threadpool.execute(move || {
            let part_start = 0;
            let part_end = files.len() - 1;

            let bytes = files
                .iter()
                .enumerate()
                .map(|(part, file)| {
                    let mut file = BufReader::new(File::open(file).unwrap());

                    if part == part_start {
                        file.seek(SeekFrom::Start(offset as u64 % BLOCK_SIZE))
                            .unwrap();
                    } else {
                        file.seek(SeekFrom::Start(0)).unwrap();
                    }

                    let bytes = file.bytes().map(|b| b.unwrap());

                    bytes
                        .take(if part == part_end {
                            (if part_start == part_end { 0 } else { offset } + size)
                                % BLOCK_SIZE as usize
                        } else {
                            BLOCK_SIZE as usize
                        })
                        .collect::<Vec<_>>()
                })
                .flatten()
                .collect::<Vec<_>>();

            reply.data(&bytes);
        });
    }

    fn release(
        &mut self,
        _req: &Request,
        _ino: u64,
        fh: u64,
        _flags: u32,
        _lock_owner: u64,
        _flush: bool,
        reply: ReplyEmpty,
    ) {
        self.file_handles.remove(&fh);
        reply.ok();
    }

    fn readdir(
        &mut self,
        _req: &Request,
        ino: u64,
        _fh: u64,
        offset: i64,
        mut reply: ReplyDirectory,
    ) {
        let file_info = self.get_file_info_from_ino(ino);

        if let Ok(file_info) = file_info {
            if offset < 2 {
                if offset == 0 {
                    reply.add(file_info.ino, 1, FileType::Directory, ".");
                }
                reply.add(
                    if file_info.parent_ino == 0 {
                        file_info.ino
                    } else {
                        file_info.parent_ino
                    },
                    2,
                    FileType::Directory,
                    "..",
                );
            }

            let mut stmt = self
                .file_db
                .prepare_cached(STMT_QUERY_BY_PARENT_INO)
                .unwrap();
            let items = stmt
                .query_map(
                    params![
                        FileInfoRow::from(FileInfo::with_parent_ino(file_info.ino)).parent_ino,
                        // The offset includes . and .., both which are not included in the
                        // database, so the SELECT offset must be adjusted. Since the offset could
                        // be negative, set it to 0 in that case.
                        0.max(offset - 2)
                    ],
                    |row| Ok(FileInfo::from(row)),
                )
                .unwrap();
            for (off, item) in items.enumerate() {
                let item = item.unwrap();

                // Here the item is added to the directory listing. It is important to note that
                // the offset parameter is quite crucial for correct function. The offset parameter
                // is used for succeeding calls to start with the next item after the last item
                // from the previous call. So, the offset parameter has to be one more than the
                // index of the current item. Furthermore, since "." and ".." have been added
                // manually as to not pollute the database with them, they also have to be handled
                // properly. They always get inserted in positions 0 and 1 respectively. If the
                // call starts at offset 0, then both of the directory hardlinks are included and
                // the offset must be increased by 2. If the starting offset is 1, then only "."
                // has been already added. For the additional "..", the offset has to be increased
                // by 1. If the offset is greater than 1, then the hardlinks have been taken care
                // of and the offset is already correct.
                reply.add(
                    item.ino,
                    offset
                        + if offset == 0 {
                            2
                        } else if offset == 1 {
                            1
                        } else {
                            0
                        }
                        + off as i64
                        + 1,
                    if item.vdir {
                        FileType::RegularFile
                    } else {
                        FileType::Directory
                    },
                    Path::new(&item.path).file_name().unwrap(),
                );
            }
            reply.ok();
        } else {
            reply.error(ENOENT);
        }
    }
}
